# -*- coding: utf-8 -*-
"""nvidia_deeplearningexamples_waveglow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/nvidia_deeplearningexamples_waveglow.ipynb

### This notebook requires a GPU runtime to run.
### Please select the menu option "Runtime" -> "Change runtime type", select "Hardware Accelerator" -> "GPU" and click "SAVE"

----------------------------------------------------------------------

# WaveGlow

*Author: NVIDIA*

**WaveGlow model for generating speech from mel spectrograms (generated by Tacotron2)**

<img src="https://pytorch.org/assets/images/waveglow_diagram.png" alt="alt" width="50%"/>
"""
import numpy as np
from scipy.io.wavfile import write

import torch

# mel = torch.load("/home/shenxz/dataset/BZNSYP/mels_test/000400.pt").unsqueeze(0).cuda()
# # mel = torch.load("/home/shenxz/dataset/BZNSYP/mels/000400.pt").unsqueeze(0)
#
# mel_path = "/home/shenxz/code/Tacotron-2-Chinese/tacotron_output/eval/mel-batch_14_sentence_0.npy"
# tmp = torch.from_numpy(np.load(mel_path))
# tmp1 = tmp.transpose(0, 1)
# tmp2 = (tmp1)*1
# mel = tmp2.unsqueeze(0).cuda()

waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')

"""will load the WaveGlow model pre-trained on [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/)

### Model Description

The Tacotron 2 and WaveGlow model form a text-to-speech system that enables user to synthesise a natural sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model (also available via torch.hub) produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech.

### Example

In the example below:
- pretrained Tacotron2 and Waveglow models are loaded from torch.hub
- Tacotron2 generates mel spectrogram given tensor represantation of an input text ("Hello world, I missed you")
- Waveglow generates sound given the mel spectrogram
- the output sound is saved in an 'audio.wav' file

To run the example you need some extra python packages installed.
These are needed for preprocessing the text and audio, as well as for display and input / output.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install numpy scipy librosa unidecode inflect librosa


"""Prepare the waveglow model for inference"""

waveglow = waveglow.remove_weightnorm(waveglow)
waveglow = waveglow.to('cuda')
waveglow.eval()

"""Load tacotron2 from PyTorch Hub"""

tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')
tacotron2 = tacotron2.to('cuda')
tacotron2.eval()

"""Now, let's make the model say *"hello world, I missed you"*"""

# text = "hello world, I missed you"
text = "There's a way to measure the acute emotional intelligence that has never gone out of style."

"""Now chain pre-processing -> tacotron2 -> waveglow"""
import time
start_time = time.time()

# # preprocessing
sequence = np.array(tacotron2.text_to_sequence(text, ['english_cleaners']))[None, :]
sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)

# run the models
with torch.no_grad():
    _, mel, _, _ = tacotron2.infer(sequence)
    # mel = torch.load("/home/shenxz/dataset/BZNSYP/mels/000400.pt").unsqueeze(0).cuda()
    print('generate mel:', time.time() - start_time)
    audio = waveglow.infer(mel)
audio_numpy = audio[0].data.cpu().numpy()
rate = 22050

print("generate raw data:", time.time() - start_time)
"""You can write it to a file and listen to it"""

write("default.wav", rate, audio_numpy)

print("generate wav:", time.time() - start_time)
"""Alternatively, play it right away in a notebook with IPython widgets"""

# from IPython.display import Audio
# Audio(audio_numpy, rate=rate)

"""### Details
For detailed information on model input and output, training recipies, inference and performance visit: [github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2) and/or [NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:tacotron_2_and_waveglow_for_pytorch)

### References

 - [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)
 - [WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/abs/1811.00002)
 - [Tacotron2 and WaveGlow on NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:tacotron_2_and_waveglow_for_pytorch)
 - [Tacotron2 and Waveglow on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2)
"""